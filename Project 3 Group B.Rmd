---
title: "Project 3"
author: "Richard Xiao & Xi Yang"
date: "2022-11-12"
output: 
  github_document:
  toc: true
params:
      channel: "lifestyle"
---
## Introduction section
This is an online news popularity data set, and dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity). We're thinking about what kind of articles are we most likely to share, and we believe there are two aspects. One is objectivity. Users can feel the content is useful and valuable. The other one is subjectivity. Users agree with the attitudes expressed in the article, and also, the emotion expressed in the article resonated with users. 

Based on the two aspects, we choose 21 variables, and they are n_tokens_title, n_tokens_content, n_unique_tokens, num_imgs, num_videos, kw_avg_min, kw_max_max, kw_avg_max, kw_avg_avg, weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday,. weekday_is_saturday, weekday_is_sunday, global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, and share.

We produce some basic analysis before we fitting the model. The purpose is to inspect the trends between different variables with respect to the number of share, and also, figure out the correlation between a few notable numeric variables. It helps the reader understand the summary or graph.

For a linear regression model, we'll use Backward stepwise and LASSO regression model. For an ensemble tree-based model, we'll fit random forest and boosted tree model.


## Data

```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(caret)
#read in data
#setwd("/Users/cathy/Desktop/project 3--Group B")
df1 <- read_csv("OnlineNewsPopularity.csv")
df1
#Create a new variable for new data channel classification. Also want to remove the old data channel variables and other variables we don't need. Also want to rename the day variables to make it easier for analysis with rename variable.
selectchannel<- paste0("data_channel_is_", params[[1]])
df <- df1 %>%
  select(-c(url, timedelta)) %>%
  mutate(log_shares = log(shares)) %>%
  select(-shares) %>% rename(monday = weekday_is_monday , tuesday = weekday_is_tuesday, wednesday = weekday_is_wednesday, thursday = weekday_is_thursday, friday =     weekday_is_friday, saturday = weekday_is_saturday, sunday = weekday_is_sunday)
df 
#a single data_channel_is_lifestyle
df_lifestyle <- df %>% filter(get(selectchannel) ==1 ) 
df_lifestyle
set.seed(100)
lifestyle_index <- createDataPartition(df_lifestyle$log_shares, p = .7, list = FALSE)
lifestyle_train <- df_lifestyle[lifestyle_index,]
lifestyle_test <- df_lifestyle[-lifestyle_index,]
lifestyle_train
lifestyle_test
```

Here, we plotted the correlation between a few notable numeric variables. 
```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(corrplot)
cor_mat <- cor(lifestyle_train %>% select(log_shares,num_imgs,num_videos,n_tokens_content,n_tokens_title,kw_avg_min,kw_max_max,kw_avg_max,kw_avg_avg,global_subjectivity,global_sentiment_polarity,global_rate_positive_words), method = "pearson")
corrplot(cor_mat, hc.order = TRUE,
type = "lower",
tl.pos = "lt",
title = "Correlation Coefficients for Bike Rental Data",
subtitle = "Correlation Coefficients for Bike Rental Data",
mar=c(0,0,2,0)
)
```
From the correlation graph, we can see that the number of shares seems to be moderately correlated to n_tokens_content, kw_min_max, kw_avg_avg, 
global_sentiment_polarity, and global_rate_positive_words. So these five variables are the most important ones from the lot, and we did further EDA, 
to see trends between different variables with respect to the number of shares.

## Summarizations

```{r echo=TRUE,eval=TRUE}
#This new dataframe converts the days into categorical values for graphing.
moddf_lifestyle1 <- lifestyle_train%>%
  mutate(day = if_else(monday == 1,"Monday",if_else(tuesday == 1,"Tuesday",if_else(wednesday == 1,"Wednesday",if_else(
thursday == 1,"Thursday",if_else(friday == 1,"Friday",if_else(saturday == 1,"Saturday", "Sunday")))))))
#Eliminates any categorical variables for use of principal component analysis
df_lifestylecontinuous <- lifestyle_train %>%select(-c(monday, tuesday, wednesday, thursday,friday, saturday, sunday, is_weekend, ))
```

### 1.
```{r echo=TRUE,eval=TRUE}
#Boxplot for log shares subdivided by days.
ggplot(moddf_lifestyle1, aes(x = day, y = log_shares, col = day)) + 
  geom_boxplot(fill="grey") + 
  geom_jitter() + 
  ylab("log(shares)") + 
  xlab("") +
  theme(axis.text.x = element_text(angle = 45)) +
  ggtitle("Boxplot for Log Shares by Day")
```

### 2.
```{r echo=TRUE,eval=TRUE}
#Scatterplot for log shares and number of images
ggplot(moddf_lifestyle1, aes(y = log_shares, x = num_imgs, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("num_hrefs") + 
  ylab("log_shares")
```

### 3.
```{r echo=TRUE,eval=TRUE}
#Scatterplot for log shares and number of videos.
ggplot(moddf_lifestyle1, aes(y = log_shares, x = num_videos, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("num_hrefs") + 
  ylab("log_shares")
```

### 4.
```{r echo=TRUE,eval=TRUE}
#Histogram for log shares 
ggplot(moddf_lifestyle1, aes(x=log_shares, fill = kw_avg_avg, color = day)) + geom_histogram(binwidth = 1, position="dodge") + xlab("Average KeyWord") + ylab("Log Shares")
```

### 5.
```{r echo=TRUE,eval=TRUE}
#Scatterplot for number of unique tokens and log shares
ggplot(moddf_lifestyle1, aes(y = log_shares, x = n_unique_tokens, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("n_unique_tokens") + 
  ylab("log_shares")
```

### 6.
```{r echo=TRUE,eval=TRUE}
#Scatterplot for number of tokens content and log shares
ggplot(moddf_lifestyle1, aes(y = log_shares, x = n_tokens_content, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("n_tokens_content") + 
  ylab("log_shares")
```

### 7.
```{r echo=TRUE,eval=TRUE}
#Scatterplot for number of token titles and log shares
ggplot(moddf_lifestyle1, aes(y = log_shares, x = n_tokens_title, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("n_tokens_title") + 
  ylab("log_shares")
```

### 8.
```{r pressure, echo=FALSE}
library(psych)
#summary statistics for this continuous dataframe. Describe function is used to get statistics like sd, mean and other stats.
lifestyle_summarystats <-describe(df_lifestylecontinuous)
arrange_lifestylesummarystats <- lifestyle_summarystats %>%arrange(desc(sd))
arrange_lifestylesummarystats
```

### 9. Numerical summary of categorical variable is_weekend

letâ€™s pull a summary of the number of shares. One of the factors that most affects the number of shares is whether the day is weekday or weekend. I wanted to look at the average, standard deviation, median and IQR values of number of shares on weekdays and weekend. If the value of average is larger , then articles tend to be shared more often.

```{r echo=TRUE,eval=TRUE}
library(tidyverse)
lifestyle_train %>%
  group_by(is_weekend) %>%
  summarise(average=mean(log_shares), median=median(log_shares), sd=sd(log_shares), IQR=IQR(log_shares))
```
### 10. Dependence of number of shares on text subjectivity

A scatter plot with the number of shares on the y-axis and the text subjectivity on the x-axis is created: we can inspect the trend of shares as a function of the text subjectivity. We know that if the value of text subjectivity is 0, it stands for the article is very objective, and value 1 stands for very subjective. If the most points distributed lower than 0.5, then articles with more objectivity tend to be shared more often. If the most points distributed larger than 0.5, then articles with more subjectivity tend to be shared more often. If the most points distributed around 0.5, then articles with more neutrality tend to be shared more often.

```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
subjectivityData <- lifestyle_train %>% group_by(global_subjectivity) %>% summarize(sharecount = mean(log_shares))
ggplot(subjectivityData, aes(x = global_subjectivity, y = sharecount, color =global_subjectivity)) +
geom_point() +
ggtitle("dependence of number of shares on text subjectivity ")
```

### 11. Dependence of number of shares on text sentiment polarity

A scatter plot with the number of shares on the y-axis and the text sentiment polarity on the x-axis is created: we can inspect the trend of shares as a function of the text sentiment polarity. We know that if the value of text sentiment polarity is greater than -1 and less than 0, it stands for the article sentiment is negative emotion. If the value of text sentiment polarity is greater than 0 and less than 1, it stands for the article sentiment is positive emotion. If the most points distributed around (-1,0), then articles with negative emotion tend to be shared more often. If the most points distributed around (0,1), then articles with positive emotion tend to be shared more often. If the most points distributed around 0, then articles with neutral emotion tend to be shared more often.


```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
sentimentpolarityData <- lifestyle_train %>% group_by(global_sentiment_polarity) %>% summarize(sharecount = mean(log_shares))
ggplot(sentimentpolarityData, aes(x = global_sentiment_polarity, y = sharecount, color =global_sentiment_polarity)) +
geom_point() +
ggtitle("dependence of number of shares on text sentiment polarity ")
```

### 12. Dependence of number of shares on positive word rate

A scatter plot with the number of shares on the y-axis and the positive word rate on the x-axis is created: we can inspect the trend of shares as a function of the positive word rate. If the points show an upward trend, then articles with more positive words tend to be shared more often. If we see a negative trend then articles with more positive words tend to be shared less often.

```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
positivewordsData <- lifestyle_train %>% group_by(global_rate_positive_words) %>% summarize(sharecount = mean(log_shares))
ggplot(positivewordsData, aes(x = global_rate_positive_words, y = sharecount), color=global_rate_positive_words) +
geom_point() +
geom_smooth(method = "lm") +
ggtitle("dependence of number of shares on positive word rate ")
```
### 13. Dependence of number of shares on negative words rate

A scatter plot with the number of shares on the y-axis and the negative words rate on the x-axis is created: we can inspect the trend of shares as a function of the negative words rate. If the points show an upward trend, then articles with more negative words tend to be shared more often. If we see a negative trend then articles with more negative words tend to be shared less often.

```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
negativewordsData <- lifestyle_train %>% group_by(global_rate_negative_words) %>% summarize(sharecount = mean(log_shares))
ggplot(negativewordsData, aes(x = global_rate_negative_words, y = sharecount)) +
geom_point() +
geom_smooth(method = "lm") +
ggtitle("dependence of number of shares on negative words rate")
```

## Modeling

For models below, I have selected the log_shares as the dependent variable, and num_imgs, num_videos, n_tokens_content, n_unique_tokens, n_tokens_title, kw_avg_min, kw_max_max, kw_avg_max, kw_avg_avg,global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, monday, tuesday, wednesday, thursday, friday, saturday, sunday as the independent variables.

```{r echo=TRUE,eval=TRUE}
library(tidyverse)
#Select variables of interest for analysis.
lifestyle_analysis <- lifestyle_train %>% select(log_shares,num_imgs,num_videos,n_tokens_content,n_unique_tokens,n_tokens_title,kw_avg_min,kw_max_max,kw_avg_max,kw_avg_avg,global_subjectivity,global_sentiment_polarity,global_rate_positive_words,global_rate_negative_words,monday,tuesday,wednesday,thursday,friday,saturday,sunday)
lifestyle_analysis
```
Here after all the models formulated below, to compare them, I am predicting the test data on this model to derive some common metrics like RMSE and R squared values that can be used to compare multiple models. For prediction, I am using predict() function, and to extract prediction metrics, I am using postResample() function.

### 1. LASSO Regression Model

Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.

```{r echo=TRUE,eval=TRUE}
library(caret)
fitLASSO <- train(log_shares ~ ., data = lifestyle_analysis,
method = "lasso",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
predLASSO <- predict(fitLASSO, newdata = lifestyle_test)
m1<-postResample(predLASSO, obs = lifestyle_test$log_shares)
m1
```
### 2. Boosted Tree Model

Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentiallyâ€”that is, each model tries to compensate for the weaknesses of its predecessor. With each iteration, the weak rules from each individual classifier are combined to form one, strong prediction rule. 

```{r echo=TRUE,eval=TRUE}
library(caret)
boostedFit <- train(log_shares ~ ., data = lifestyle_analysis, method = "gbm",trControl = trainControl(method = "cv" , number = 10),
                    preProcess = c("center", "scale"),
                    tuneGrid = expand.grid(n.trees = c(25, 50, 100, 150, 200),
                                           interaction.depth = 1:4,
                                           shrinkage = 0.1,
                                           n.minobsinnode = 10)
                    )
pred_boosted <- predict(boostedFit, newdata = lifestyle_test)
m2<-postResample(pred_boosted, lifestyle_test$log_shares)
m2
```

### 3. Forward Fitting Model
```{r echo=TRUE,eval=TRUE}
library(caret)
fit_forward <- train(log_shares ~., data = lifestyle_analysis,  method = "leapForward", preProcess = c("center", "scale"),trControl = trainControl(method = "cv", number = 10))
fit_forward_prediction <- predict(fit_forward, newdata = lifestyle_test)
m3<-postResample(fit_forward_prediction, lifestyle_test$log_shares)
m3
```

### 4. Random Forest Model
```{r}
fit_forest <- train(log_shares ~ ., data = lifestyle_analysis, method = "treebag",trControl = trainControl(method = "cv" , number = 10),preProcess = c("center", "scale"),mtry = c(1:21))
pred_forest <- predict(fit_forest, newdata = lifestyle_test)
m4<-postResample(pred_forest, lifestyle_test$log_shares)
m4
```


## Comparison
```{r echo=TRUE,eval=TRUE}
LASSO<- tibble(model = c("LASSO"), RMSE = c(m1[[1]]), Rsquared = c(m1[[2]]))

boostedTree<- tibble(model = c("boosted"), RMSE = c(m2[[1]]), Rsquared = c(m2[[2]]))

farward<- tibble(model = c("farward"), RMSE = c(m3[[1]]), Rsquared = c(m3[[2]]))

randomForest<- tibble(model = c("randomForest"), RMSE = c(m4[[1]]), Rsquared = c(m4[[2]]))

comparison<- rbind(LASSO, boostedTree, farward, randomForest)
comparison
```
From the above table we can say that, among the LASSO, boosted tree, forward and random forest models, the best model is LASSO model since it has the least RMSE and maximum Rsquared value as compared to the other three models.


## Automation
#install.packages("purrr")
```{r, eval=FALSE}
library(purrr)
library(tidyverse)
#get unique teams
channelIDs <- data.frame("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
channelIDs
#create filenames
output_file <- paste0(channelIDs, ".md")
#create a list for each team with just the team name parameter
params<- lapply(channelIDs, FUN = function(x){list(channel = x)})
params
#put into a data frame
reports <- tibble(output_file, params)
reports
```

```{r, eval=FALSE}
library(rmarkdown)
#need to use x[[1]] to get at elements since tibble doesn't simplify
apply(reports, MARGIN = 1,
FUN = function(x){
render(input = "Project-3--Group-B -.Rmd", output_file = x[[1]], params = x[[2]])
})
```



