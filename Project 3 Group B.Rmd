---
title: "Project 3"
author: "Richard Xiao"
date: "2022-11-11"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(dplyr)
library(psych)
library(corrplot)
```

### Introduction section

This is an online news popularity data set, and dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity). We're thinking about what kind of articles are we most likely to share, and we believe there are two aspects. One is objectivity. Users can feel the content is useful and valuable. The other one is subjectivity. Users agree with the attitudes expressed in the article, and also, the emotion expressed in the article resonated with users. 

Based on the two aspects, we choose 26 variables, and they are n_tokens_title, n_tokens_content, n_unique_tokens, num_imgs, num_videos, kw_avg_min, kw_max_max, kw_avg_max, kw_avg_avg, weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday,. weekday_is_saturday, weekday_is_sunday, is_weekend, LDA_00: Closeness to LDA topic 0, global_subjectivity, global_sentiment_polarity, global_rate_positive_words, global_rate_negative_words, rate_positive_words, rate_negative_words, avg_positive_polarity, avg_negative_polarity.

We produce some basic analysis before we fitting the model. The purpose is to inspect the trends between different variables with respect to the number of share, and also, figure out the correlation between a few notable numeric variables. It helps the reader understand the summary or graph.

For a linear regression model, we'll use Backward stepwise and LASSO regression model. For an ensemble tree-based model, we'll fit random forest and boosted tree model.

## Testing


```{r cars}
df <- read_csv("./OnlineNewsPopularity.csv")
#Create a new variable for new data channel classification. Also want to remove the old data channel variables and other variables we don't need. Also want to rename the day variables to make it easier for analysis with rename variable.
df <- df %>%
  mutate(data_channel = if_else(
    data_channel_is_bus == 1,
    "Business",
    if_else(
      data_channel_is_entertainment == 1,
      "Entertainment",
      if_else(
        data_channel_is_lifestyle == 1,
        "Lifestyle",
        if_else(
          data_channel_is_socmed == 1,
          "Socmed",
          if_else(data_channel_is_tech == 1, "Tech", "World")
        )
      )
    )
  )) %>% 
  select(-c(url, timedelta, data_channel_is_bus, data_channel_is_entertainment,
            data_channel_is_socmed, data_channel_is_tech, data_channel_is_world,
            data_channel_is_lifestyle)) %>%
  mutate(log_shares = log(shares)) %>%
  select(-shares) %>% rename(Monday = weekday_is_monday , Tuesday = weekday_is_tuesday, Wednesday = weekday_is_wednesday, Thursday = weekday_is_thursday, Friday = weekday_is_friday, Saturday = weekday_is_saturday, Sunday = weekday_is_sunday)
  
  
df_lifestyle <- df %>% filter(data_channel == "Lifestyle") 

lifestyle_index <- createDataPartition(df_lifestyle$log_shares, p = .7, list = FALSE)
lifestyle_train <- df_lifestyle[lifestyle_index,]
lifestyle_test <- df_lifestyle[-lifestyle_index,]

#This new dataframe converts the days into categorical values for graphing.

moddf_lifestyle1 <- lifestyle_train%>%
  mutate(day = if_else(Monday == 1,"Monday",if_else(Tuesday == 1,"Tuesday",if_else(Wednesday == 1,"Wednesday",if_else(
Thursday == 1,"Thursday",if_else(Friday == 1,"Friday",if_else(Saturday == 1,"Saturday", "Sunday")))))))




#Boxplot for log shares subdivided by days.

ggplot(moddf_lifestyle1, aes(x = day, y = log_shares, col = day)) + 
  geom_boxplot(fill="grey") + 
  geom_jitter() + 
  ylab("log(shares)") + 
  xlab("") +
  theme(axis.text.x = element_text(angle = 45)) +
  ggtitle("Boxplot for Log Shares by Day")

#Scatterplot for log shares and number of images
ggplot(moddf_lifestyle1, aes(y = log_shares, x = num_imgs, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("num_hrefs") + 
  ylab("log_shares")

#Scatterplot for log shares and number of videos.
ggplot(moddf_lifestyle1, aes(y = log_shares, x = num_videos, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("num_hrefs") + 
  ylab("log_shares")


#Histogram for log shares 
ggplot(moddf_lifestyle1, aes(x=log_shares, fill = kw_avg_avg, color = day)) + geom_histogram(binwidth = 1, position="dodge") + xlab("Average KeyWord") + ylab("Log Shares")

#Scatterplot for number of unique tokens and log shares
ggplot(moddf_lifestyle1, aes(y = log_shares, x = n_unique_tokens, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("n_unique_tokens") + 
  ylab("log_shares")

#Scatterplot for number of tokens content and log shares
ggplot(moddf_lifestyle1, aes(y = log_shares, x = n_tokens_content, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("n_tokens_content") + 
  ylab("log_shares")

#Scatterplot for number of token titles and log shares
ggplot(moddf_lifestyle1, aes(y = log_shares, x = n_tokens_title, color = day)) + 
  geom_point(stat = "identity", position = "jitter") + 
  geom_smooth( method = "lm")  + 
  xlab("n_tokens_title") + 
  ylab("log_shares")
```


## Correlation Analysis

According to the outputted correlation graphs, the log shares response doesn't seem to correlate with any of the prediction variables.
```{r}
#Subdivided into four chunks to analyze the log shares correlation with the lifestyle analysis predictor variables.

#First Correlation plot

first_corr_lifestyle_analysis <- lifestyle_train %>% select(log_shares,num_imgs,num_videos,n_tokens_content,n_unique_tokens,n_tokens_title)

cor_mat <- cor(first_corr_lifestyle_analysis, method = "pearson")
corrplot(cor_mat, hc.order = TRUE,
type = "lower",
tl.pos = "lt",
title = "Correlation Coefficients for Channel Data",
subtitle = "Correlation Coefficients for Channel Data",
mar=c(0,0,2,0)
)

#Second Correlation plot

second_corr_lifestyle_analysis <- lifestyle_train %>% select(log_shares,kw_avg_min,kw_max_max,kw_avg_max,kw_avg_avg)

cor_mat <- cor(second_corr_lifestyle_analysis, method = "pearson")
corrplot(cor_mat, hc.order = TRUE,
type = "lower",
tl.pos = "lt",
title = "Correlation Coefficients for Channel Data",
subtitle = "Correlation Coefficients for Channel Data",
mar=c(0,0,2,0))


#Third Correlation Plot

third_corr_lifestyle_analysis <- lifestyle_train %>% select(log_shares,global_subjectivity,global_sentiment_polarity,global_rate_positive_words,global_rate_negative_words)

cor_mat <- cor(third_corr_lifestyle_analysis, method = "pearson")
corrplot(cor_mat, hc.order = TRUE,
type = "lower",
tl.pos = "lt",
title = "Correlation Coefficients for Channel Data",
subtitle = "Correlation Coefficients for Channel Data",
mar=c(0,0,2,0))


#Fourth Correlation Plot

fourth_corr_lifestyle_analysis <- lifestyle_train %>% select(log_shares,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday)

cor_mat <- cor(fourth_corr_lifestyle_analysis, method = "pearson")
corrplot(cor_mat, hc.order = TRUE,
type = "lower",
tl.pos = "lt",
title = "Correlation Coefficients for Channel Data",
subtitle = "Correlation Coefficients for Channel Data",
mar=c(0,0,2,0))


```


## Summary Statistics
According to the output of our table, the kw_max_max variable has the highest standard deviation,median and mean compared to our other variables. Also, according to our contingency table with comparison of days and data channel, most amount of lifestyle articles were published in Wednesday while the least amount of articles were published on Saturday.
```{r pressure, echo=FALSE}
#summary statistics for this continuous dataframe. Describe function is used to get statistics like sd, mean and other stats.

#Eliminates any categorical variables for use of principal component analysis
df_lifestylecontinuous <- lifestyle_train %>%select(-c(Monday, Tuesday, Wednesday, Thursday,Friday, Saturday, Sunday, is_weekend, data_channel))

lifestyle_summarystats <-describe(df_lifestylecontinuous)
arrange_lifestylesummarystats <- lifestyle_summarystats %>%arrange(desc(sd))
lifestyle_summarystats
arrange_lifestylesummarystats
table(moddf_lifestyle1$day,moddf_lifestyle1$data_channel)
```
### Summarizations

1.
```{r echo=TRUE,eval=TRUE}
library(tidyverse)
lifestyle_train %>%
  group_by(is_weekend) %>%
  summarise(avg=mean(log_shares), med=median(log_shares), var=var(log_shares))
```
2.
```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
subjectivityData <- lifestyle_train %>% group_by(global_subjectivity) %>% summarize(sharecount = mean(log_shares))
ggplot(subjectivityData, aes(x = global_subjectivity, y = sharecount, color =global_subjectivity)) +
geom_point() +
ggtitle("dependence of numbers of shares on Text subjectivity ")
```

3.
```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
sentimentpolarityData <- lifestyle_train %>% group_by(global_sentiment_polarity) %>% summarize(sharecount = mean(log_shares))
ggplot(sentimentpolarityData, aes(x = global_sentiment_polarity, y = sharecount, color =global_sentiment_polarity)) +
geom_point() +
ggtitle("dependence of numbers of shares on Text sentiment polarity ")
```

4.
```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
positivewordsData <- lifestyle_train %>% group_by(global_rate_positive_words) %>% summarize(sharecount = mean(log_shares))
ggplot(positivewordsData, aes(x = global_rate_positive_words, y = sharecount), color=global_rate_positive_words) +
geom_point() +
geom_smooth(method = "lm") +
ggtitle("dependence of numbers of shares on Rate of positive words in the content ")
```
5.
```{r echo=TRUE,eval=TRUE}
library(tidyverse)
library(ggplot2)
negativewordsData <- lifestyle_train %>% group_by(global_rate_negative_words) %>% summarize(sharecount = mean(log_shares))
ggplot(negativewordsData, aes(x = global_rate_negative_words, y = sharecount)) +
geom_point() +
geom_smooth(method = "lm") +
ggtitle("dependence of numbers of shares on Rate of Rate of negative words in the content ")
```


```{r}
#Random forest model

#Select variables of interest for analysis.
lifestyle_analysis <- lifestyle_train %>% select(log_shares,num_imgs,num_videos,n_tokens_content,n_unique_tokens,n_tokens_title,kw_avg_min,kw_max_max,kw_avg_max,kw_avg_avg,global_subjectivity,global_sentiment_polarity,global_rate_positive_words,global_rate_negative_words,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday)

cor_mat <- cor(lifestyle_analysis, method = "pearson")
corrplot(cor_mat, hc.order = TRUE,
type = "lower",
tl.pos = "lt",
title = "Correlation Coefficients for Channel Data",
subtitle = "Correlation Coefficients for Channel Data",
mar=c(0,0,2,0)
)


fit_forest <- train(log_shares ~ ., data = lifestyle_analysis, method = "treebag",trControl = trainControl(method = "cv" , number = 10),preProcess = c("center", "scale"),mtry = c(1:21))

pred_forest <- predict(fit_forest, newdata = lifestyle_test)
postResample(pred_forest, lifestyle_test$log_shares)


```



## Screeplot and Biplot

For the first chunk, the biplots show that the 1st PC doesn't have a relationship with number of videos. The second PC does not have a relationship with unique tokens and token content.According to the screeplot, two PCs would be sufficient.

For the second chunk, the biplots shows PC 1 having a relationship with every variable except for kw_avg_min. PC1 doesn't really have a strong relationship with the number of tokens title. For PC2, it doesn't have a strong relationship with number of tokens title. According to the screeplot, 2 or 3 pcs would be sufficient.

The biplot for the last chunk shows PC 1 with a relationship on every variable except for kw_avg_min. PC1 doesn't really have a strong relationship with the number of tokens title. For PC2, it doesn't have a strong relationship with number of tokens title. According to the screeplot, 2 or 3 pcs would be sufficient.
```{r}
#For this portion, I split the principal component aspect into 3 chunks since including all of the variables in one graph would make it unreadable. 

#First chunk of code for screeplot and biplot. 

df_no_shares <- df_lifestylecontinuous %>%
  select(num_imgs,num_videos,n_tokens_content,n_unique_tokens)
#Creating PC's along with center and scaling variables
PCs <- prcomp(df_no_shares, center = TRUE, scale = TRUE)
#Creating screeplots
par(mfrow = c(1,2))
plot(PCs$sdev^2/sum(PCs$sdev^2), xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(cumsum(PCs$sdev^2/sum(PCs$sdev^2)), xlab = "Principal Component",
ylab = "Cum. Prop of Variance Explained", ylim = c(0, 1), type = 'b')
#Selecting only the PC's up to a 80% variance explained threshold using caret
PCs_eighty <- preProcess(df_no_shares, method = c("center","scale", "pca"), thresh = .8)
#Creating a data frame with just my PC's, day variables, and log_shares to use later as a regression
df_PC <- predict(PCs_eighty, newdata = df_no_shares)
#Monday is excluded to avoid multicollinearity
df_PC <- df_PC %>%
  bind_cols(log_shares = df_lifestylecontinuous$log_shares,Tuesday = lifestyle_train$Tuesday, 
            Wednesday = lifestyle_train$Wednesday, Thursday = lifestyle_train$Thursday, Friday = lifestyle_train$Friday,
            Saturday = lifestyle_train$Saturday, Sunday = lifestyle_train$Sunday)
screeplot(PCs, type = "lines")
biplot(PCs)


#Second chunk of code for screeplot and biplot

df_no_shares <- df_lifestylecontinuous %>%
  select(n_tokens_title,kw_avg_min,kw_max_max,kw_avg_max,kw_avg_avg)
#Creating PC's along with center and scaling variables
PCs <- prcomp(df_no_shares, center = TRUE, scale = TRUE)
#Creating screeplots
par(mfrow = c(1,2))
plot(PCs$sdev^2/sum(PCs$sdev^2), xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(cumsum(PCs$sdev^2/sum(PCs$sdev^2)), xlab = "Principal Component",
ylab = "Cum. Prop of Variance Explained", ylim = c(0, 1), type = 'b')
#Selecting only the PC's up to a 80% variance explained threshold using caret
PCs_eighty <- preProcess(df_no_shares, method = c("center","scale", "pca"), thresh = .8)
#Creating a data frame with just my PC's, day variables, and log_shares to use later as a regression
df_PC <- predict(PCs_eighty, newdata = df_no_shares)
#Monday is excluded to avoid multicollinearity
df_PC <- df_PC %>%
  bind_cols(log_shares = df_lifestylecontinuous$log_shares,Tuesday = lifestyle_train$Tuesday, 
            Wednesday = lifestyle_train$Wednesday, Thursday = lifestyle_train$Thursday, Friday = lifestyle_train$Friday,
            Saturday = lifestyle_train$Saturday, Sunday = lifestyle_train$Sunday)
screeplot(PCs, type = "lines")
biplot(PCs)


#Third Chunk


df_no_shares <- df_lifestylecontinuous %>%
  select(global_subjectivity,global_sentiment_polarity,global_rate_positive_words,global_rate_negative_words)
#Creating PC's along with center and scaling variables
PCs <- prcomp(df_no_shares, center = TRUE, scale = TRUE)
#Creating screeplots
par(mfrow = c(1,2))
plot(PCs$sdev^2/sum(PCs$sdev^2), xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(cumsum(PCs$sdev^2/sum(PCs$sdev^2)), xlab = "Principal Component",
ylab = "Cum. Prop of Variance Explained", ylim = c(0, 1), type = 'b')
#Selecting only the PC's up to a 80% variance explained threshold using caret
PCs_eighty <- preProcess(df_no_shares, method = c("center","scale", "pca"), thresh = .8)
#Creating a data frame with just my PC's, day variables, and log_shares to use later as a regression
df_PC <- predict(PCs_eighty, newdata = df_no_shares)
#Monday is excluded to avoid multicollinearity
df_PC <- df_PC %>%
  bind_cols(log_shares = df_lifestylecontinuous$log_shares,Tuesday = lifestyle_train$Tuesday, 
            Wednesday = lifestyle_train$Wednesday, Thursday = lifestyle_train$Thursday, Friday = lifestyle_train$Friday,
            Saturday = lifestyle_train$Saturday, Sunday = lifestyle_train$Sunday)
screeplot(PCs, type = "lines")
biplot(PCs)

```

### Modeling
```{r echo=TRUE,eval=TRUE}
library(tidyverse)
#Select variables of interest for analysis.
lifestyle_analysis <- lifestyle_train %>% select(log_shares,num_imgs,num_videos,n_tokens_content,n_unique_tokens,n_tokens_title,kw_avg_min,kw_max_max,kw_avg_max,kw_avg_avg,global_subjectivity,global_sentiment_polarity,global_rate_positive_words,global_rate_negative_words,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday)
lifestyle_analysis
```

1. LASSO Regression Model
```{r echo=TRUE,eval=TRUE}
library(caret)
fitLASSO <- train(log_shares ~ ., data = lifestyle_analysis,
method = "lasso",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 10)
)
predLASSO <- predict(fitLASSO, newdata = lifestyle_test)
m1<-postResample(predLASSO, obs = lifestyle_test$log_shares)
m1
```
2. Boosted Tree Model
```{r echo=TRUE,eval=TRUE}
library(caret)
boostedFit <- train(log_shares ~ ., data = lifestyle_analysis, method = "gbm",trControl = trainControl(method = "cv" , number = 10),
                    preProcess = c("center", "scale"),
                    tuneGrid = expand.grid(n.trees = c(25, 50, 100, 150, 200),
                                           interaction.depth = 1:4,
                                           shrinkage = 0.1,
                                           n.minobsinnode = 10)
                    )
pred_boosted <- predict(boostedFit, newdata = lifestyle_test)
m2<-postResample(pred_boosted, lifestyle_test$log_shares)
m2
```
3.

```{r}
#Forward fitting model

fit_forward <- train(log_shares ~., data = lifestyle_analysis,  method = "leapForward", preProcess = c("center", "scale"),trControl = trainControl(method = "cv", number = 10))

fit_forward_prediction <- predict(fit_forward, newdata = lifestyle_test)
m3 <- postResample(fit_forward_prediction, lifestyle_test$log_shares)


```


4.
```{r}
#Random forest model
#Select variables of interest for analysis.
lifestyle_analysis <- lifestyle_train %>% select(log_shares,num_imgs,num_videos,n_tokens_content,n_unique_tokens,n_tokens_title,kw_avg_min,kw_max_max,kw_avg_max,kw_avg_avg,global_subjectivity,global_sentiment_polarity,global_rate_positive_words,global_rate_negative_words,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday)
fit_forest <- train(log_shares ~ ., data = lifestyle_analysis, method = "treebag",trControl = trainControl(method = "cv" , number = 10),preProcess = c("center", "scale"),mtry = c(1:21))
pred_forest <- predict(fit_forest, newdata = lifestyle_test)
m4<-postResample(pred_forest, lifestyle_test$log_shares)
m4
```
### Model Comparison

According to the comparison of these models, the random forest would be the best model since it has the lowest RMSE and highest R squared value, with the forward model being the worst performer.
```{r}
LASSO<- tibble(model = c("LASSO"), RMSE = c(m1[[1]]), Rsquared = c(m1[[2]]))

boostedTree<- tibble(model = c("boosted"), RMSE = c(m2[[1]]), Rsquared = c(m2[[2]]))

forward<-  tibble(model = c("forward"), RMSE = c(m3[[1]]), Rsquared = c(m3[[2]]))

randomForest<- tibble(model = c("randomForest"), RMSE = c(m4[[1]]), Rsquared = c(m4[[2]]))

comparison<- rbind(LASSO, boostedTree, forward, randomForest)
comparison
```

